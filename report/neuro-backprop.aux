\relax 
\citation{sac_senn}
\citation{code}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Different neuron types. }}{1}}
\newlabel{fig:neurons}{{1}{1}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {Draft of a biological pyramidal neuron.}}}{1}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {Abstracted and simplified computational model of a pyramidal neuron.}}}{1}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(c)}{\ignorespaces {Draft of a biological interneuron.}}}{1}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(d)}{\ignorespaces {Abstracted and simplified computational model of an interneuron.}}}{1}}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1}Multicompartment-Neurons}{1}}
\newlabel{eq:soma}{{1}{1}}
\newlabel{eq:steady}{{2}{2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2}PyraL-Network (PyraLNet) }{2}}
\newlabel{eq:dyn_hidden}{{6}{2}}
\newlabel{eq:apical}{{9}{2}}
\newlabel{eq:dyn_out}{{12}{2}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces An abstracted cortical microcircuit capable of approximating the backpropagation algorithm. Note that the notation differs from the one used in the main text, namely $W^{PP}_{k,k-1}\equiv W^{up}_k$ and $W^{PP}_{k+1,k}\equiv W^{down}_k$.}}{3}}
\newlabel{fig:network}{{2}{3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3}Learning-Rules}{3}}
\newlabel{eq:out_stead}{{14}{3}}
\citation{urb_senn}
\newlabel{eq:basal_pyr}{{17}{4}}
\newlabel{eq:dend_int}{{20}{4}}
\newlabel{eq:delta_up}{{21}{4}}
\newlabel{eq:delta_ap}{{23}{4}}
\newlabel{eq:W_pi}{{26}{4}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.3.1}Self-predicting state}{4}}
\newlabel{eq:sps_pi}{{29}{5}}
\newlabel{eq:sps_ip}{{30}{5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.4}Approximation of the Error-Backpropagation Algorithm \footnote {A complete derivation can be found in the original paper.}}{5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.5}Steady-State-Approximation of the Network Dynamics (SteadNet)}{5}}
\newlabel{chap:steadnet}{{1.5}{5}}
\@writefile{toc}{\contentsline {section}{\numberline {2}Experimental Results}{6}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Basics}{6}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.1}Stability and Fix-Points}{6}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces \textbf  {Upper row}: Evolution of soma potentials of a simple 3-layer network with three neurons in total. Columns represent different network configurations and initial conditions and weights are kept fixed during the simulation. \textbf  {Lower row}: Derivative of the hidden pyramidal soma potential $u^P_{hid}$ fully expressed through $u^P_{hid}$ via the steady-state solution of the other soma potential (see main text). \textbf  {Details: } $g_A = g_{som} = 0.8$, $g_D = g_B = 1$, $g_l = 0.1$, $r_{in} = u^{target} = 1$, $dt = 0.001$}}{6}}
\newlabel{fig:stability}{{3}{6}}
\citation{code}
\newlabel{eq:fix_out}{{33}{7}}
\newlabel{eq:fix_inter}{{34}{7}}
\newlabel{eq:fix_pyr}{{35}{7}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.2}Basic Regression Task and Emergence of Self-Predicting-State}{7}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces The networks solves a basic regression task in which it has to mimic a randomly initialized (forward-)teaching network.}}{8}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {Output layer soma potentials and target potentials before training.}}}{8}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {Output layer soma potentials and target potentials after training.}}}{8}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(c)}{\ignorespaces {MSE validation error during training.}}}{8}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(d)}{\ignorespaces {Emergence of self-predicting-state during training starting from randomly initialized connections.}}}{8}}
\newlabel{fig:mimic}{{4}{8}}
\citation{code}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.3}Steady-State-Approximation}{9}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Validation of the steady-state network approximation.}}{10}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {Deviation\footnotemark\nobreakspace {}of the simulated from the approximation soma potentials, when plasticity is switched off.}}}{10}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {Deviation of the simulated from the approximation soma potentials, when plasticity is switched off, but the network is initialized in the self-predicting state.}}}{10}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(c)}{\ignorespaces {Deviation of the simulated from the approximation weights, when plasticity is switched on.}}}{10}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(d)}{\ignorespaces {Deviation of the simulated from the approximation weights, when plasticity is switched on, but the network is initialized in the self-predicting state.}}}{10}}
\newlabel{fig:stead}{{5}{10}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}YinYang-Task}{10}}
\citation{sac_senn}
\citation{code}
\@writefile{toc}{\contentsline {section}{\numberline {3}Conclusion}{11}}
\bibdata{ref}
\bibcite{code}{1}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Impact of different parameters on classification performance for the YinYang-task.}}{12}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {Test set accuracy versus learning-lag for different $t_{pattern}$. See main text for parameter details.}}}{12}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {Test set accuracy versus learning-lag for different $t_{pattern}$, but with \textit {reset\_deltas} switched on.}}}{12}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(c)}{\ignorespaces {Test set accuracy versus pattern transition time constant $\tau _0$.}}}{12}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(d)}{\ignorespaces {Test set accuracy versus plasticity time constant $\tau _w$.}}}{12}}
\newlabel{fig:yinyang_vary}{{6}{12}}
\bibcite{sac_senn}{2}
\bibcite{urb_senn}{3}
\bibstyle{plain}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Solving the YinYang-task from a randomly initialized network.}}{13}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {Validation output potential MSE and classification accuracy during training on the left. The right side shows the accuracy achieved on the validation and test dataset. The picture is obtained on the test set. Note that training lasted for 55 epochs.}}}{13}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {Evolution of the self-predicting state during training.}}}{13}}
\newlabel{fig:yinyang_sps}{{7}{13}}
