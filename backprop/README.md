# File Structure
| paths           | description
|-----------------|---------
| `eval`          | contains plots generated by eval.py (for report)
| `plots`	  | contains plots generated by PyraLNet.py and SteadNet.py for the different 'local' (non-cluster version) tasks
| `runs`          | results of the different runs (parameter sweeps) performed on the cluster
| `ann.py`        | simple artificial neural network, don't bother so much
| `build_df.py`   | builds pandas dataframe from the result file of a runs
| `check_jobs.py` | checks on the state of the jobs of a run and may restart failed once
| `Dataset.py`    | generate YinYang-, Bars- dataset
| `eval.py`       | generate plots from runs-data for report
| `job_pyral.sh`  | script to be called by moab -> starts 20 subprocesses calling PyraLNet.py
| `job_stead.sh`  | script to be called by moab -> starts 20 subprocesses calling SteadNet.py
| `PyraLNet.py`   | implementation of the Sacramento, Senn paper (full simulation), example tasks and cluster tasks
| `SteadNet.py`   | implementation of the Sacramento, Senn paper (steady-state approximation) and example tasks
| `Scheduler.py`  | edit to schedule a run (parameter sweep) and call on the cluster to start run
| `Stability.py`  | jupyter notebook investigating the stability properties of a tiny network consisting of 3 neurons 
| `test.conf`     | a trial config file for (PyraL/Stead)-Net.py for testing


# Run Organisation and Scheduling on the Cluster
A parameter sweep, a run, is scheduled in Scheduler.py. After specifying the name of the run, a corresponding directory is created in runs/, containing a results/, conf/ and tmp/ sub-directory.
Then for each of the parameter sets a config file is created and saved to config/, identified by a unique name. Then these sub-runs are bundled into jobs, where each job contains 20 sub-runs. Per job a `index`.job file is saved to tmp/. It contains a path to the results/ directory and the 20 paths to the .conf-files associated with this jobs. Furthermore, tmp/ also contains a list relating the job index (or ids) and the job-id returned by moab. This is necessary to keep track of the job states (check_jobs.py) during execution. Next, per job, moab is called with the right job_***.sh script, to be executed by the cluster, and the .job file as argument. The job-shell script will then call PyraLNet or SteadNet in a subprocesses for each config file given in the .job file it received as an argument by moab. Finally, PyraLNet or SteadNet script will execute with the specific parameters it was started with and writes the results to results/results.txt. 
